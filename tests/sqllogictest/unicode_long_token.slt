statement ok
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    passage TEXT
);

statement ok
INSERT INTO documents (passage) VALUES 
('PostgreSQL is a powerful, open-source object-relational database system. It has over 15 years of active development.'),
('Full-text search is a technique for searching in plain-text documents or textual database fields. PostgreSQL supports this with tsvector.'),
('BM25 is a ranking function used by search engines to estimate the relevance of documents to a given search query.'),
('PostgreSQL provides many advanced features like full-text search, window functions, and more.'),
('Search and ranking in databases are important in building effective information retrieval systems.'),
('The BM25 ranking algorithm is derived from the probabilistic retrieval framework.'),
('Full-text search indexes documents to allow fast text queries. PostgreSQL supports this through its GIN and GiST indexes.'),
('The PostgreSQL community is active and regularly improves the database system.'),
('Relational databases such as PostgreSQL can handle both structured and unstructured data.'),
('Effective search ranking algorithms, such as BM25, improve search results by understanding relevance.');

# We need to use random string to test, because btree will compress long text.
statement ok
INSERT INTO documents (passage) SELECT array_to_string(array(SELECT chr((65 + (random() * 25))::int) FROM generate_series(1, 10000)), ''); 

statement ok
SELECT create_tokenizer('documents_tokenizer', $$
tokenizer = 'unicode'
table = 'documents'
column = 'passage'
$$);

statement ok
ALTER TABLE documents ADD COLUMN embedding bm25vector;

statement ok
UPDATE documents SET embedding = tokenize(passage, 'documents_tokenizer');

statement ok
CREATE INDEX documents_embedding_bm25 ON documents USING bm25 (embedding bm25_ops);

statement ok
SELECT id, passage, embedding <&> to_bm25query('documents_embedding_bm25', 'Postgresql', 'documents_tokenizer') AS rank
FROM documents
ORDER BY rank
LIMIT 10;

statement ok
SELECT drop_tokenizer('documents_tokenizer');

statement ok
DROP TABLE documents;
